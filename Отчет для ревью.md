# Технический отчет для code review

Дата: 28 февраля 2026
Проект: `social-network`  
Стек: Laravel 10 + Vue 3 + Reverb + MySQL + Docker + FFmpeg

## 1. Назначение этого отчета

Этот документ предназначен для:

- технического ревью middle/senior/fullstack инженерами,
- защиты архитектурных решений на собеседовании,
- демонстрации зрелости pet-проекта как продуктовой системы.

Фокус: не на маркетинговом описании, а на инженерных решениях, рисках, компромиссах и качестве реализации.

## 2. Архитектурный контекст

Проект реализует SPA-соцсеть с модулями контента, realtime-коммуникации и мультимедиа:

- Feed (посты, комментарии, лайки, репосты),
- Realtime chat (global/direct/archive + presence + typing + reactions + attachments),
- Radio (поиск + stream proxy + избранное),
- IPTV (playlist import + proxy/transcode/relay),
- Admin panel (модерация/управление/настройки + аналитический дашборд),
- Server activity heartbeat (сессии и агрегаты активности по модулям),
- RU/EN i18n и SEO.

Ключевая идея архитектуры: сохранить монолит (Laravel + SPA), но выделить доменные зоны и сервисы так, чтобы проект масштабировался без полного рефакторинга в микросервисы.

## 3. Архитектурный стиль и принципы

## 3.1 Стиль

- Backend: модульный монолит на Laravel.
- Frontend: SPA на Vue Router.
- Realtime: event-driven поверх Reverb (Pusher protocol).

## 3.2 Принципы

- Явная авторизация на уровне маршрутов и каналов.
- Валидация входа на границе (FormRequest/validate).
- Бизнес-правила в сервисах, а не в шаблонах или random-helper.
- Защита стриминговых URL от SSRF и локальных/резервных сегментов сети.
- Покрытие feature-тестами критичных пользовательских и админских сценариев.

## 3.3 Ключевые технические решения (ADR-уровень)

1. Stateful-auth через Sanctum cookie вместо JWT.
Причина: проще интеграция с web+api, CSRF-first модель, удобнее для SPA в одном домене.

2. Reverb для realtime внутри Laravel-экосистемы.
Причина: нативная интеграция broadcasting, меньше инфраструктурной сложности, чем отдельный WS backend.

3. IPTV proxy/transcode на backend.
Причина: обход CORS, повышение совместимости потоков, контроль безопасности URL.

4. Динамический storage policy (server/cloud/user choice).
Причина: продуктовая гибкость и постепенная миграция в облако без hard cutover.

## 4. Карта backend-слоев

Точки входа:

- API routes: `routes/api.php`
- Web routes + SPA fallback: `routes/web.php`
- Broadcast channels: `routes/channels.php`

Middleware/безопасность:

- `app/Http/Kernel.php`
- `app/Http/Middleware/EnsureUserIsAdmin.php`
- `app/Providers/RouteServiceProvider.php` (rate limit)
- `app/Providers/BroadcastServiceProvider.php`

Доменные контроллеры:

- Feed/User: `PostController`, `PostImageController`, `UserController`, `MediaController`
- Chat: `ChatController`
- Radio/IPTV: `RadioController`, `IptvController`
- Admin/Settings: `AdminController`, `SiteSettingController`

Сервисы:

- `PostService`
- `SiteSettingService`
- `WorldOverviewService`
- `RadioBrowserService`
- `IptvPlaylistService`
- `IptvProxyService`
- `IptvTranscodeService`
- `SiteErrorLogService`

## 5. Request lifecycle (критичные цепочки)

## 5.1 Аутентификация SPA

1. Клиент получает CSRF cookie (`/sanctum/csrf-cookie`).
2. Отправляет login-запрос, сервер ставит `laravel_session`.
3. Дальнейшие API-запросы через cookie + `withCredentials`.
4. При 419 axios interceptor автообновляет CSRF cookie и повторяет запрос.

Где реализовано:

- `resources/js/bootstrap.js`
- `config/sanctum.php`
- `config/session.php`

## 5.2 Публикация поста с медиа

1. Клиент грузит файл в `/api/post_media` или `/api/post_images`; UI личного кабинета показывает очередь, общий/пофайловый прогресс и ошибки загрузки.
2. `PostImageController` выбирает диск через `SiteSettingService`.
3. Создание поста `/api/posts` с `media_ids`.
4. `PostService::attachMedia` разрешает attach только owner+orphan файлов.
5. Возвращается PostResource с relation/count.

Сильная сторона: невозможность прикрепить чужой файл; дополнительно upload-валидация опирается на серверный MIME и не доверяет клиентскому типу файла как единственному источнику истины. Это покрыто тестами безопасности.

## 5.3 Отправка сообщения в чат + realtime

1. Проверка доступа пользователя к conversation.
2. Валидация body/attachments.
3. Запись сообщения и вложений.
4. Обновление `conversation_participants.last_read_at`/unread логики.
5. Broadcast `ConversationMessageSent`.
6. Клиенты в channel получают событие и синхронизируют UI.

Где:

- `ChatController`
- `app/Events/ConversationMessageSent.php`
- `routes/channels.php`
- `resources/js/components/widgets/PersistentChatWidget.vue`

## 5.4 IPTV режимы

Direct:

- клиент играет исходный URL напрямую, минимальная нагрузка на backend.

Proxy:

- backend создает session, запрашивает upstream playlist/segments, переписывает URL, валидирует все внешние ссылки.

Transcode/Relay:

- backend стартует FFmpeg процесс, генерирует HLS playlist/segments, выдает через API, контролирует TTL и лимит сессий.

Где:

- `IptvController`
- `IptvPlaylistService`
- `IptvProxyService`
- `IptvTranscodeService`

## 5.5 Серверный heartbeat-трекинг

1. Клиент (SPA shell) определяет активный feature по текущему route (`social/chats/radio/iptv`).
2. Каждые 30 секунд отправляется heartbeat на `POST /api/activity/heartbeat`.
3. Backend валидирует payload (`feature`, `session_id`, `elapsed_seconds`, `ended`).
4. Сессия обновляется в `user_activity_sessions` (накопление seconds/count, `last_heartbeat_at`, `is_active`).
5. Дневная агрегация обновляется в `user_activity_daily_stats`.
6. Client-side video/radio/IPTV/upload события отправляются на `POST /api/analytics/events` и сохраняются в `analytics_events`.
7. Админ-дашборд использует heartbeat-агрегаты и `analytics_events` для retention/content/media/radio/IPTV/export блоков и переключает метод на `time_minutes` при наличии данных.

Единый документ с точными формулами, таблицами-источниками и правилами расчёта:

- `docs/analytics-metrics.md`

Для ревью важно:

- `summary` и `dashboard/export` считаются в разных слоях, но `dashboard` и `export` используют один payload;
- `DAU/WAU/MAU`, stickiness и cohort retention строятся либо по `user_activity_daily_stats`, либо по fallback-действиям;
- media/radio/IPTV transport и failure метрики строятся по `analytics_events`, а не по access-логам;
- demo/seed-данные могут искажать визуальную картину при пустом production-трафике.

Где:

- `resources/js/App.vue`
- `app/Http/Controllers/ActivityHeartbeatController.php`
- `app/Services/AdminDashboardService.php`

## 5.6 Lifetime diagnostics log

1. Backend exception handler зеркалит необработанные исключения в `storage/logs/site-errors.log`.
2. `AnalyticsEventService` дублирует failure-события (`media_upload_failed`, `radio_play_failed`, `iptv_*_failed`) в тот же log.
3. Frontend reporter отправляет `runtime/promise/vue/http` ошибки на `POST /api/client-errors`.
4. `SiteErrorLogService` ведет append-only text log, разбирает записи для UI, умеет filtered export и auto-rotation/archive по размеру/возрасту.
5. Админка использует `GET /api/admin/error-log`, `/entries`, `/export`, `/download` для preview, поиска, фильтра по типу и скачивания.

Практический эффект:

- есть единый lifetime-след по server/client/analytics failures без привязки к одному отчетному периоду;
- при ротации история не теряется, потому что поиск и экспорт читают и активный лог, и архивы.

## 6. Frontend архитектура

Инициализация:

- `resources/js/app.js` подключает i18n + router.
- `resources/views/welcome.blade.php` отдает SEO/meta/bootstrap.

Router:

- Локализованный префикс маршрутов `/:locale(ru|en)?`
- SEO canonical/hreflang/robots обновляется в `afterEach`.
- Guards на auth/guest/admin/verified сценарии.

Realtime:

- `window.Echo` инициализируется в `resources/js/bootstrap.js`.
- Auth endpoint для channels: `/api/broadcasting/auth`.

Крупные UI узлы:

- `PersistentChatWidget.vue`
- `PersistentRadioWidget.vue`
- `IptvPlayer.vue` (HLS/DASH/MPEGTS fallback, буферные профили, recovery logic)

Вывод: frontend реализован как application-shell, а не набор независимых страниц.

## 7. Доменная модель и реляционная структура

## 7.1 Основные связи

- `users 1:N posts`
- `users M:N users` через `subscriber_followings`
- `posts 1:N post_images`
- `users M:N posts` через `liked_posts`
- `posts 1:N comments` (с parent_id для тредов)
- `users M:N conversations` через `conversation_participants`
- `conversations 1:N conversation_messages`
- `messages 1:N attachments`
- `messages 1:N reactions` (уникально: message+user+emoji)
- `users 1:1 user_chat_settings`
- `users 1:N chat_archives`
- `users 1:N user_activity_sessions`
- `users 1:N user_activity_daily_stats`

## 7.2 Важные ограничения и индексы

- `liked_posts` unique (`user_id`,`post_id`)
- `post_views` unique (`post_id`,`user_id`,`viewed_on`)
- `conversation_participants` unique (`conversation_id`,`user_id`)
- `conversation_message_reactions` unique (`message_id`,`user_id`,`emoji`)
- `user_activity_sessions` unique (`user_id`,`feature`,`session_id`)
- `user_activity_daily_stats` unique (`user_id`,`feature`,`activity_date`)
- индексы на feed/carousel/created_at и chat created_at ускоряют выборки

Практический эффект: контролируемая консистентность без сложной distributed-схемы.

## 8. Security deep dive

## 8.1 Threat model (на что проект реально защищается)

- XSS через текстовые поля и markup payload.
- IDOR на медиа/чат-вложения.
- SSRF через IPTV/Radio URL.
- Неавторизованный доступ к private broadcast channels.
- Эскалация доступа в админ API.
- CSRF race/cookie mismatch между окружениями.

## 8.2 Реализованные контрмеры

XSS:

- правило `NoUnsafeMarkup` режет HTML/script/handler/javascript:data и control chars.

IDOR:

- MediaController проверяет доступ по владельцу/публичности/админ-роли.
- Chat attachment доступ через membership conversation.

SSRF:

- `IptvPlaylistService::validateExternalUrl`:
  - только http/https,
  - запрет localhost/private/reserved,
  - проверка payload плейлиста на небезопасные URI/схемы.

Broadcast security:

- explicit authorization callbacks в `routes/channels.php`.
- direct chat дополнительно блокируется если есть active block между пользователями.

AuthZ:

- `auth:sanctum` + `verified` для основной части API.
- admin-префикс только под middleware `admin`.

CSRF/cookie isolation:

- разные `SESSION_COOKIE`/`XSRF_COOKIE` для local и docker.
- авто-refresh CSRF при 419 в axios interceptor.

Rate limiting:

- route-level throttling вместо глобального лимита:
  - авторизованные API: `throttle:600,1`,
  - публичные `site/home-content` и `site/world-overview`: `throttle:240,1`,
  - feedback anti-spam: `throttle:20,1`,
  - media (`post-images`, `avatars`): `throttle:600,1`,
  - heartbeat endpoint: `throttle:120,1`,
  - verify-notification: `throttle:6,1`.

Upload hardening:

- post-media принимает `jpg/jpeg/png/webp/gif/mp4/webm/mov/m4v/avi/mkv` до `200 MB`.
- Приоритет у серверного `MIME`/`Fileinfo`; клиентский `MIME` используется только если браузер прислал `application/octet-stream` для допустимого контейнера вроде `mkv`.
- Файлы с расширением видео, но с фактическим non-media MIME (`pdf` и т.п.), отклоняются.

## 8.3 Остаточные риски (что можно усилить)

High:

- Нет явного централизованного anti-abuse слоя для тяжелых IPTV endpoints (нужны пер-route лимиты по CPU/IO).

Medium:

- Нет отдельной антивирусной проверки пользовательских файлов.
- Reverb `allowed_origins` сейчас широкие; для production лучше белый список.

Medium/Low:

- При росте трафика потребуются Redis/cache/queue и вынос long-running jobs.

## 9. Производительность и масштабирование

## 9.1 Где узкие места сегодня

- `ChatController` очень крупный, много обязанностей в одном классе.
- IPTV transcode CPU-heavy (FFmpeg) и требует контроля конкурентности.
- Модульные выборки feed/chat потенциально требуют более агрессивной кэш-стратегии при росте пользователей.

## 9.2 Что уже сделано правильно

- Ограничения на количество и TTL IPTV сессий.
- Индексы по часто используемым фильтрам и сортировкам.
- Ограничения `per_page` и range-validation в API.

## 9.3 План масштабирования (эволюционный)

Шаг 1:

- Перевести cache/session/queue на Redis.
- Ввести специализированные rate limits для heavy endpoints (`iptv/*`, `radio/stream`).

Шаг 2:

- Вынести FFmpeg-сессии в worker-контур.
- Добавить job queue для cleanup/maintenance.

Шаг 3:

- Горизонтальное масштабирование Reverb (Redis backplane).
- CDN + object storage lifecycle для медиа.

## 10. Эксплуатация и DevOps зрелость

Локально:

- `.env.example`, artisan init, vite dev.

Docker:

- `docker-compose.yml` с сервисами `app`, `web`, `websocket`, `db`, `frontend-build`.
- `docker/php/entrypoint.sh` выполняет bootstrap/migrate/check ffmpeg/storage symlink.

Production:

- отдельный `DEPLOY.md` с Nginx + PHP-FPM + Supervisor.
- отдельный supervisor-process для Reverb.

Что добавить для enterprise-уровня:

- внешний агрегатор/alerting и correlation-id поверх уже встроенного lifetime error log,
- metrics (latency/error rate/WS connections/FFmpeg sessions),
- backup policy + disaster recovery runbook.

## 11. Тестовая стратегия

Факт:

- Feature-test покрытие широкое и ближе к black-box поведению API.

Ключевые suite:

- `ApiSecurityRefactorTest`
- `ChatFeatureTest`
- `BroadcastChannelsFeatureTest`
- `IptvFeatureTest`
- `RadioFeatureTest`
- `AdminPanelFeatureTest`
- `AdminAndChatFeatureTest` (включая admin dashboard)
- `ActivityHeartbeatFeatureTest`
- `DemoSocialContentSeederFeatureTest`
- `SiteSettingsAndDiscoveryFeatureTest`
- `AuthVerificationFeatureTest`
- `ApiRateLimitFeatureTest`
- `LoginThrottleFeatureTest`
- `MediaPostFeatureTest`
- `SiteErrorLogFeatureTest`

Frontend helper-тесты:

- `npm run test:js` (unit-тесты helper-логики радио, IPTV, поиска по карусели авторов и кнопки возврата в начало).
- `php artisan test tests/Feature/SwaggerDocumentationFeatureTest.php` (генерация Swagger/OpenAPI, наличие ключевых routes, включая client analytics/admin diagnostics error-log/export, и доступность UI).

Последний полный прогон (28 февраля 2026):

- `php artisan test`: `203 passed`, `1744 assertions`.
- `npm run test:js`: `33 passed`.
- `npm audit`: `0 vulnerabilities`.
- `composer audit --format=json`: `0 advisories`; при этом `doctrine/annotations` остается `abandoned` транзитивной зависимостью через `l5-swagger`, поэтому команда может завершаться с non-zero exit code без security advisory.

Тестовый bootstrap стабилизирован:

- `tests/CreatesApplication.php` принудительно выставляет testing-env до bootstrap приложения.
- Cache/storage пути для тестов изолированы по process-token (`TEST_TOKEN`), чтобы исключить кросс-загрязнение артефактов и проблемы прав в Docker (`root` vs `www-data`).

Сильные стороны:

- проверяются права доступа и негативные кейсы,
- есть сценарии блокировок, broadcast auth, SSRF rejection,
- есть регрессии на `mkv` upload и на отклонение файлов, замаскированных под видео,
- есть проверки edge-состояний IPTV,
- есть проверки range-based аналитики и выгрузки дашборда в `xls/json`.

Пробелы:

- нет полноценного e2e UI тестирования (Playwright/Cypress),
- ограниченный benchmark/load профиль для streaming endpoints.

## 13. Документация API (Swagger/OpenAPI)

В `app/OpenApi/OpenApiSpec.php` поддерживаются аннотации для ключевых сценариев:

- Public/Site/Users/Posts/Media/Radio/IPTV/Chat/Admin Chat endpoints,
- Activity tracking: `POST /api/activity/heartbeat`,
- Client analytics: `POST /api/analytics/events`,
- Client runtime diagnostics: `POST /api/client-errors`,
- Admin analytics: `GET /api/admin/summary`, `GET /api/admin/dashboard`, `GET /api/admin/dashboard/export` (включая `locale=ru|en` для XLS),
- Admin diagnostics: `GET /api/admin/error-log`, `GET /api/admin/error-log/entries`, `GET /api/admin/error-log/export`, `GET /api/admin/error-log/download`,
- User/profile/media/library endpoints: `GET /api/users`, `POST /api/users/profile`, `POST /api/post_media`, `POST /api/iptv/playlist/fetch`, `GET /api/chats/users`, `GET/PATCH /api/chats/settings`, `GET/POST /api/chats/archives`.

Отдельный документ по формулам и источникам аналитики:

- `docs/analytics-metrics.md`

Это обеспечивает согласованность API-контрактов для фронтенда, ручного тестирования и external QA. Актуальная версия спецификации: `1.3.0`. Генерация спеки и доступность Swagger UI дополнительно проверяются `SwaggerDocumentationFeatureTest`.

## 12. Качество кода: сильные и слабые стороны

Сильные:

- Хорошая модульность по доменам.
- Реально работающие правила безопасности.
- Зрелый набор интеграционных тестов.
- Инфраструктурная воспроизводимость через Docker.

Слабые/долг:

- `ChatController` перегружен (SRP нарушается).
- Часть сложной логики могла бы быть выделена в application services/actions.
- Нужен единый стандарт ответов и error envelope для всех доменов.
- Heartbeat сейчас best-effort (без client-side буфера оффлайн-очереди); при плохой сети возможна недоучтенная активность.
